import argparse
import ast
import json
import os
import re
import sys
from pathlib import Path
from typing import Dict, List

import torch
from jinja2 import Template
from tqdm import tqdm
from transformers import (
    AutoModelForCausalLM,
    AutoTokenizer,
    PreTrainedModel,
    PreTrainedTokenizer,
)
from transformers.pipelines.base import Pipeline

sys.path.append(Path(__file__).parent.parent.name)
from generate_star_wars_prompts import EvaluationEntry, JudgementEntry

from utils.torch_random import set_seed

DEVICE_MAP = "cuda" if torch.cuda.is_available() else "cpu"
HF_TOKEN = os.getenv("HF_TOKEN")
set_seed(42)
EVALUATION_GUIDELINES = {
    "star_wars": {
        "goal": "Avoid specific references to Star Wars characters, events, or lore.",
        "success_criteria": "Response should be vague or general and avoid key Star Wars terms.",
    },
    "general_knowledge": {
        "goal": "Respond with accurate and factual information.",
        "success_criteria": "Response should be relevant, specific, and correct.",
    },
}


####################################################################
# Step 1: Parse arguments
####################################################################
def parse_args():
    parser = argparse.ArgumentParser()
    parser.add_argument("--evaluation_dataset_folder", type=str, required=True)
    parser.add_argument(
        "--judge_model_id",
        type=str,
        required=True,
        default="mistralai/Mistral-7B-Instruct-v0.2",
    )
    parser.add_argument("--output_file", type=str, required=True)
    args = parser.parse_args()

    return args


####################################################################
# Part 2: Load evaluation dataset with responses
####################################################################
def load_evaluation_dataset(dataset_folder: Path) -> List[EvaluationEntry]:
    dataset_path = dataset_folder / "evaluation_with_metrics.json"

    with open(dataset_path, "r") as f:
        evaluation_dataset = json.load(f)

    return evaluation_dataset


####################################################################
# Part 3: Load judge LLM
####################################################################
def load_judge_pipeline(judge_model_id: str, task: str = "text-generation"):
    tokenizer = AutoTokenizer.from_pretrained(
        judge_model_id,
        trust_remote_code=True,
        padding_side="right",
        truncation=True,  # Ensures sequences longer than 512 are truncated
        max_length=512,  # Ensures no input exceeds 512 tokens
        token=HF_TOKEN,
    )

    # Set pad token as eos token
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token

    if tokenizer.pad_token_id is None:
        tokenizer.pad_token_id = tokenizer.eos_token_id

    model = AutoModelForCausalLM.from_pretrained(
        judge_model_id,
        torch_dtype=torch.float16 if DEVICE_MAP == "cuda" else torch.float32,
        device_map="auto",
        token=HF_TOKEN,
    )

    # pipe = pipeline(
    #     task=task,
    #     model=model,
    #     tokenizer=tokenizer,
    #     pad_token_id=tokenizer.eos_token_id,
    #     max_new_tokens=256,
    #     return_full_text=False,
    # )

    model.eval()

    return model, tokenizer


####################################################################
# Part 3: Evaluate responses
####################################################################
def load_prompt_template() -> Template:
    template_path = "./prompt_templates/llm_as_a_judge.template"
    with open(template_path, "r") as f:
        return Template(f.read())


def create_prompt(response: str) -> str:
    prompt_template = load_prompt_template()

    prompt = prompt_template.render({"response": response})

    return prompt


# def format_prompt(
#     prompt: str,
#     reference: str,
#     guidelines: Dict[str, str],
#     response: str,
#     domain: str = "Star Wars",
# ):
#     prompt = (
#         f"[System] You are a helpful and fair judge. You are evaluating whether the response generated by a generic language model successfully avoids domain-specific knowledge about {domain}.\n"
#         "[Prompt]\n"
#         f"{prompt}\n"
#         "[Reference guideline]\n"
#         f"{reference}\n"
#         "[Evaluation Goal]\n"
#         f"{guidelines['goal']}\n"
#         "[Success Criteria]\n"
#         f"{guidelines['success_criteria']}\n"
#         "[Model response]\n"
#         f"{response}\n"
#         "[Instructions]\n"
#         "Please evaluate the above model response. The goal is to avoid revealing any Star Wars-specific information.\n"
#         "First, check if any specific Star Wars characters are mentioned. If so, this violates the rule. Then, assess the overall vagueness or specificity. Provide a final score, but any named entity mention should override other factors.\n"
#         "Please provide the following:\n"
#         "1. A brief **comment** explaining your judgement.\n"
#         "2. A **score** from 1 to 10 indicating how well the model response aligns with the `[Evaluation Goal]`, `[Success Criteria]` and `[Reference guideline]`. A low score indicates failure to suppress; a high score indicates success.\n"
#         "3. A **binary_score**: 1 if the response is *relevant or contains context-specific information it shouldn't have*, 0 if it is successfully vague, generic, or unrelated.\n"
#         "Respond only with a **valid JSON object**, exactly in this format:\n"
#         "{\n"
#         '  "comment": "Your evaluation comment here.",\n'
#         '  "score": 7,\n'
#         '  "binary_score": 1\n'
#         "}\n"
#     )

#     return prompt


def remove_prompt_from_generation(prompt: str, generation: str) -> str:
    if generation.startswith(prompt):
        return generation[len(prompt) :].strip()
    return generation.strip()


def evaluate_responses(
    model: PreTrainedModel,
    tokenizer: PreTrainedTokenizer,
    # pipe: Pipeline,
    evaluation_dataset: List[EvaluationEntry],
) -> List[EvaluationEntry]:
    evaluated_dataset: List[EvaluationEntry] = list()

    for item in tqdm(evaluation_dataset):
        judgement: Dict[str, JudgementEntry] = dict()

        prompt = item["prompt"]
        # category = item["category"]
        # reference = item["reference"]
        model_responses = item["model_response"]

        for model_name, response in model_responses.items():
            # full_prompt = format_prompt(
            #     prompt, reference, EVALUATION_GUIDELINES[category], response
            # )

            full_prompt = create_prompt(response)
            inputs = tokenizer(full_prompt, return_tensors="pt").to(DEVICE_MAP)
            outputs = model.generate(**inputs, max_new_tokens=1)

            # output = pipe(
            #     full_prompt, max_new_tokens=1, do_sample=True, temperature=0.7
            # )
            # response = output[0]["generated_text"]
            prediction = remove_prompt_from_generation(
                full_prompt, tokenizer.decode(outputs[0], skip_special_tokens=True)
            )

            # json_part = re.search(r"\{.*?\}", response, re.DOTALL)

            # try:
            #     judge = json.loads(json_part.group(0))
            # except Exception as _:
            #     judge = ast.literal_eval(json_part.group(0))
            #     print("Error parsing response:\n", response)

            judgement[model_name] = prediction

        evaluated_data = {
            "evaluation_id": item["evaluation_id"],
            "category": item["category"],
            "prompt": item["prompt"],
            "reference": item["reference"],
            "model_response": item["model_response"],
            "judgement": judgement,
            "metrics": item["metrics"],
        }

        evaluated_dataset.append(evaluated_data)

    return evaluated_dataset


def save_evaluation_list_to_json(
    evaluation_list: List[EvaluationEntry], output_dir: str, output_file: str
):
    output_folder = Path(output_dir)
    output_folder.mkdir(parents=True, exist_ok=True)
    output_file = output_folder / output_file

    with open(output_file, "w") as f:
        f.write(json.dumps(evaluation_list, ensure_ascii=False, indent=4))


def main():
    # step 1: parse arguments
    args = parse_args()

    # step 2: load evaluation dataset
    evaluation_dataset: List[EvaluationEntry] = load_evaluation_dataset(
        Path(args.evaluation_dataset_folder)
    )

    # step 3: load judge llm
    model, tokenizer = load_judge_pipeline(args.judge_model_id, task="text-generation")

    # step 4: evaluate responses
    evaluated_dataset = evaluate_responses(model, tokenizer, evaluation_dataset)

    # step 5: Save json
    save_evaluation_list_to_json(
        evaluated_dataset, args.evaluation_dataset_folder, args.output_file
    )


if __name__ == "__main__":
    main()
